"""
Yapay Zeka Model EÄŸitim Scripti
Isolation Forest algoritmasÄ± ile anomali tespiti modeli eÄŸitir.
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score
import joblib
import os
import sys
from config import MODEL_CONFIG, DATA_GENERATION

def load_and_prepare_data():
    """
    CSV dosyasÄ±ndan veriyi yÃ¼kler ve model iÃ§in hazÄ±rlar.
    
    Returns:
        X: Ã–zellik matrisi
        y: GerÃ§ek anomali etiketleri (doÄŸrulama iÃ§in)
    """
    data_file = DATA_GENERATION['output_file']
    
    if not os.path.exists(data_file):
        print(f"âŒ Veri dosyasÄ± bulunamadÄ±: {data_file}")
        print("ğŸ’¡ Ã–nce 'python veri_uret.py' komutunu Ã§alÄ±ÅŸtÄ±rÄ±n!")
        sys.exit(1)
    
    print(f"ğŸ“‚ Veri yÃ¼kleniyor: {data_file}")
    df = pd.read_csv(data_file)
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    
    print(f"âœ… {len(df):,} kayÄ±t yÃ¼klendi")
    
    # Model iÃ§in Ã¶zellikleri seÃ§ (sensÃ¶r deÄŸerleri)
    feature_columns = [
        'toprak_direnci',
        'kacak_akim',
        'toprak_potansiyel',
        'toprak_nemi',
        'toprak_sicakligi',
        'korozyon_seviyesi'
    ]
    
    X = df[feature_columns].values
    y = df['anomali'].values  # GerÃ§ek etiketler (doÄŸrulama iÃ§in)
    
    return X, y, df


def train_isolation_forest(X_train, contamination=0.1):
    """
    Isolation Forest modeli eÄŸitir.
    
    Args:
        X_train: EÄŸitim verisi
        contamination: Beklenen anomali oranÄ± (0.1 = %10)
    
    Returns:
        model: EÄŸitilmiÅŸ model
        scaler: Veri Ã¶lÃ§eklendirici
    """
    print("\nğŸ”§ Model eÄŸitimi baÅŸlÄ±yor...")
    print(f"   Algoritma: Isolation Forest")
    print(f"   Beklenen anomali oranÄ±: {contamination*100:.1f}%")
    
    # Veriyi Ã¶lÃ§eklendir (normalize et)
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_train)
    
    # Isolation Forest modeli oluÅŸtur
    model = IsolationForest(
        contamination=contamination,
        random_state=42,
        n_estimators=100,
        max_samples='auto',
        n_jobs=-1
    )
    
    # Modeli eÄŸit
    print("   â³ Model eÄŸitiliyor...")
    model.fit(X_scaled)
    print("   âœ… Model eÄŸitimi tamamlandÄ±!")
    
    return model, scaler


def evaluate_model(model, scaler, X_test, y_test):
    """
    Model performansÄ±nÄ± deÄŸerlendirir.
    
    Args:
        model: EÄŸitilmiÅŸ model
        scaler: Veri Ã¶lÃ§eklendirici
        X_test: Test verisi
        y_test: GerÃ§ek etiketler
    """
    print("\nğŸ“Š Model deÄŸerlendirmesi yapÄ±lÄ±yor...")
    
    # Test verisini Ã¶lÃ§eklendir
    X_test_scaled = scaler.transform(X_test)
    
    # Tahmin yap
    predictions = model.predict(X_test_scaled)
    
    # Isolation Forest: -1 = anomali, 1 = normal
    # Bizim etiketlerimiz: 1 = anomali, 0 = normal
    # DÃ¶nÃ¼ÅŸtÃ¼rme yap
    predictions_binary = (predictions == -1).astype(int)
    
    # Metrikleri hesapla
    f1 = f1_score(y_test, predictions_binary)
    precision = precision_score(y_test, predictions_binary, zero_division=0)
    recall = recall_score(y_test, predictions_binary, zero_division=0)
    
    print("\nğŸ“ˆ Performans Metrikleri:")
    print(f"   â€¢ F1 Skoru: {f1:.4f}")
    print(f"   â€¢ Kesinlik (Precision): {precision:.4f}")
    print(f"   â€¢ DuyarlÄ±lÄ±k (Recall): {recall:.4f}")
    
    # Confusion Matrix
    cm = confusion_matrix(y_test, predictions_binary)
    print("\nğŸ”¢ Confusion Matrix:")
    print(f"   GerÃ§ek Normal / Tahmin Normal: {cm[0,0]}")
    print(f"   GerÃ§ek Normal / Tahmin Anomali: {cm[0,1]}")
    print(f"   GerÃ§ek Anomali / Tahmin Normal: {cm[1,0]}")
    print(f"   GerÃ§ek Anomali / Tahmin Anomali: {cm[1,1]}")
    
    # DetaylÄ± rapor
    print("\nğŸ“‹ DetaylÄ± SÄ±nÄ±flandÄ±rma Raporu:")
    print(classification_report(y_test, predictions_binary, 
                              target_names=['Normal', 'Anomali'],
                              zero_division=0))
    
    return {
        'f1_score': f1,
        'precision': precision,
        'recall': recall,
        'confusion_matrix': cm
    }


def predict_anomaly(model, scaler, sensor_data):
    """
    Yeni bir sensÃ¶r verisi iÃ§in anomali tahmini yapar.
    
    Args:
        model: EÄŸitilmiÅŸ model
        scaler: Veri Ã¶lÃ§eklendirici
        sensor_data: SensÃ¶r verisi (dict veya array)
    
    Returns:
        is_anomaly: True/False
        anomaly_score: Anomali skoru (-1 ile 1 arasÄ±, dÃ¼ÅŸÃ¼k deÄŸer = anomali)
    """
    # Dict ise array'e Ã§evir
    if isinstance(sensor_data, dict):
        feature_columns = [
            'toprak_direnci',
            'kacak_akim',
            'toprak_potansiyel',
            'toprak_nemi',
            'toprak_sicakligi',
            'korozyon_seviyesi'
        ]
        sensor_array = np.array([[sensor_data[col] for col in feature_columns]])
    else:
        sensor_array = sensor_data.reshape(1, -1)
    
    # Ã–lÃ§eklendir
    sensor_scaled = scaler.transform(sensor_array)
    
    # Tahmin yap
    prediction = model.predict(sensor_scaled)[0]
    anomaly_score = model.score_samples(sensor_scaled)[0]
    
    is_anomaly = (prediction == -1)
    
    return is_anomaly, anomaly_score


def calculate_risk_score(anomaly_score, sensor_data):
    """
    Anomali skorundan risk puanÄ± hesaplar (0-100 arasÄ±).
    
    Args:
        anomaly_score: Modelin verdiÄŸi anomali skoru
        sensor_data: SensÃ¶r verisi (risk hesaplamasÄ± iÃ§in)
    
    Returns:
        risk_score: 0-100 arasÄ± risk puanÄ±
    """
    # Anomali skorunu normalize et (dÃ¼ÅŸÃ¼k skor = yÃ¼ksek risk)
    # Isolation Forest skorlarÄ± genellikle -0.5 ile 0.5 arasÄ±ndadÄ±r
    normalized_score = (anomaly_score + 0.5) / 1.0  # 0-1 arasÄ±na normalize et
    
    # Risk puanÄ±: DÃ¼ÅŸÃ¼k anomaly_score = YÃ¼ksek risk
    base_risk = (1 - normalized_score) * 100
    
    # SensÃ¶r deÄŸerlerine gÃ¶re ek risk faktÃ¶rleri
    risk_factors = 0
    
    # Toprak direnci yÃ¼ksekse risk artar
    if sensor_data.get('toprak_direnci', 0) > 10:
        risk_factors += 20
    
    # KaÃ§ak akÄ±m yÃ¼ksekse risk artar
    if sensor_data.get('kacak_akim', 0) > 20:
        risk_factors += 15
    
    # Korozyon yÃ¼ksekse risk artar
    if sensor_data.get('korozyon_seviyesi', 0) > 50:
        risk_factors += 10
    
    risk_score = min(base_risk + risk_factors, 100)
    
    return round(risk_score, 2)


def save_model(model, scaler, model_path=None):
    """
    EÄŸitilmiÅŸ modeli ve scaler'Ä± kaydeder.
    
    Args:
        model: EÄŸitilmiÅŸ model
        scaler: Veri Ã¶lÃ§eklendirici
        model_path: KayÄ±t yolu
    """
    if model_path is None:
        model_path = MODEL_CONFIG['model_path']
    
    # KlasÃ¶r yoksa oluÅŸtur
    model_dir = os.path.dirname(model_path)
    if model_dir and not os.path.exists(model_dir):
        os.makedirs(model_dir)
    
    # Modeli kaydet
    joblib.dump({
        'model': model,
        'scaler': scaler
    }, model_path)
    
    print(f"\nğŸ’¾ Model kaydedildi: {model_path}")


def load_model(model_path=None):
    """
    KaydedilmiÅŸ modeli yÃ¼kler.
    
    Args:
        model_path: Model yolu
    
    Returns:
        model: YÃ¼klenen model
        scaler: YÃ¼klenen scaler
    """
    if model_path is None:
        model_path = MODEL_CONFIG['model_path']
    
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Model dosyasÄ± bulunamadÄ±: {model_path}")
    
    data = joblib.load(model_path)
    return data['model'], data['scaler']


def main():
    """
    Ana eÄŸitim fonksiyonu
    """
    print("=" * 60)
    print("ğŸ¤– Yapay Zeka Model EÄŸitimi")
    print("=" * 60)
    
    # Veriyi yÃ¼kle
    X, y, df = load_and_prepare_data()
    
    # Veriyi eÄŸitim ve test olarak ayÄ±r (%80 eÄŸitim, %20 test)
    split_idx = int(len(X) * 0.8)
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    print(f"\nğŸ“Š Veri BÃ¶lÃ¼nmesi:")
    print(f"   â€¢ EÄŸitim: {len(X_train):,} kayÄ±t")
    print(f"   â€¢ Test: {len(X_test):,} kayÄ±t")
    
    # Modeli eÄŸit
    contamination = MODEL_CONFIG['contamination']
    model, scaler = train_isolation_forest(X_train, contamination)
    
    # Modeli deÄŸerlendir
    metrics = evaluate_model(model, scaler, X_test, y_test)
    
    # Modeli kaydet
    save_model(model, scaler)
    
    # Ã–rnek tahmin
    print("\nğŸ§ª Ã–rnek Tahmin Testi:")
    sample_data = {
        'toprak_direnci': 3.5,
        'kacak_akim': 5.0,
        'toprak_potansiyel': 1.0,
        'toprak_nemi': 40.0,
        'toprak_sicakligi': 20.0,
        'korozyon_seviyesi': 15.0
    }
    is_anomaly, anomaly_score = predict_anomaly(model, scaler, sample_data)
    risk_score = calculate_risk_score(anomaly_score, sample_data)
    
    print(f"   Ã–rnek Veri: {sample_data}")
    print(f"   Anomali Tespiti: {'âš ï¸ EVET' if is_anomaly else 'âœ… HAYIR'}")
    print(f"   Anomali Skoru: {anomaly_score:.4f}")
    print(f"   Risk PuanÄ±: {risk_score:.2f}/100")
    
    print("\n" + "=" * 60)
    print("âœ… Model eÄŸitimi baÅŸarÄ±yla tamamlandÄ±!")
    print("=" * 60)


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"\nâŒ Hata oluÅŸtu: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

